{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# import cvxopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SVMRegression:\n",
    "#     def __init__(self, C=1, kernel='linear', degree=3, gamma='scale'):\n",
    "#         self.C = C\n",
    "#         self.kernel = kernel\n",
    "#         self.degree = degree\n",
    "#         self.gamma = gamma\n",
    "#         self.support_vectors_ = None\n",
    "#         self.support_vectors_y_ = None\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         n_samples, n_features = X.shape\n",
    "\n",
    "#         # Compute the kernel matrix\n",
    "#         K = np.zeros((n_samples, n_samples))\n",
    "#         for i in range(n_samples):\n",
    "#             for j in range(n_samples):\n",
    "#                 K[i,j] = self.kernel_function(X[i], X[j])\n",
    "\n",
    "#         # # Define the quadratic programming problem\n",
    "#         # P = np.outer(y, y) * K\n",
    "#         # q = -np.ones(n_samples)\n",
    "#         # G = np.vstack((np.eye(n_samples)*-1, np.eye(n_samples)))\n",
    "#         # h = np.hstack((np.zeros(n_samples), np.ones(n_samples) * self.C))\n",
    "\n",
    "#         # # Solve the quadratic programming problem using an optimizer\n",
    "        \n",
    "#         # P, q, G, h = matrix(P), matrix(q), matrix(G), matrix(h)\n",
    "#         # solvers.options['show_progress'] = False\n",
    "#         # solution = solvers.qp(P, q, G, h)\n",
    "\n",
    "#         # # Get the Lagrange multipliers\n",
    "#         # lagrange_multipliers = np.array(solution['x']).flatten()\n",
    "\n",
    "\n",
    "#      # Initialize Lagrange multipliers and error cache\n",
    "#         lagrange_multipliers = np.zeros(n_samples)\n",
    "#         error_cache = np.zeros(n_samples)\n",
    "\n",
    "#         # Train the SVM regression model using SMO algorithm\n",
    "#         num_changed = 0\n",
    "#         examine_all = True\n",
    "#         iteration = 0\n",
    "#         while num_changed > 0 or examine_all:\n",
    "#             num_changed = 0\n",
    "#             if examine_all:\n",
    "#                 for i in range(n_samples):\n",
    "#                     num_changed += self.examine_example(i, X, y, K, lagrange_multipliers, error_cache)\n",
    "#             else:\n",
    "#                 non_bound_indices = np.where((lagrange_multipliers > 0) & (lagrange_multipliers < self.C))[0]\n",
    "#                 for i in non_bound_indices:\n",
    "#                     num_changed += self.examine_example(i, X, y, K, lagrange_multipliers, error_cache)\n",
    "#             if examine_all:\n",
    "#                 examine_all = False\n",
    "#             elif num_changed == 0:\n",
    "#                 examine_all = True\n",
    "#             iteration += 1\n",
    "#             if iteration >= self.max_iter:\n",
    "#                 break\n",
    "\n",
    "\n",
    "\n",
    "#         # Select the support vectors\n",
    "#         threshold = 1e-5\n",
    "#         support_vectors = lagrange_multipliers > threshold\n",
    "#         self.support_vectors_ = X[support_vectors]\n",
    "#         self.support_vectors_y_ = y[support_vectors]\n",
    "#         self.lagrange_multipliers_ = lagrange_multipliers[support_vectors]\n",
    "\n",
    "#         # Compute the bias term\n",
    "#         self.intercept_ = np.mean(self.support_vectors_y_ - np.sum(self.lagrange_multipliers_ * self.support_vectors_y_ * K[support_vectors, :], axis=0))\n",
    "\n",
    "#     def kernel_function(self, x1, x2):\n",
    "#         if self.kernel == 'linear':\n",
    "#             return np.dot(x1, x2)\n",
    "#         elif self.kernel == 'poly':\n",
    "#             return (np.dot(x1, x2) + 1) ** self.degree\n",
    "#         elif self.kernel == 'rbf':\n",
    "#             return np.exp(-self.gamma * np.linalg.norm(x1 - x2) ** 2)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "#     def examine_example(self, i, X, y, K, lagrange_multipliers, error_cache):\n",
    "#         y_i = y[i]\n",
    "#         alpha_i = lagrange_multipliers[i]\n",
    "#         E_i = self.predict(X[i]) - y_i\n",
    "#         r_i = E_i * y_i\n",
    "\n",
    "#         if ((r_i < -self.tol and alpha_i < self.C) or (r_i > self.tol and alpha_i > 0)):\n",
    "#             non_bound_indices = np.where((lagrange_multipliers > 0) & (lagrange_multipliers < self.C))[0]\n",
    "#             if len(non_bound_indices) > 1:\n",
    "#                 j = self.select_second_example(i, E_i, error_cache)\n",
    "#                 if self.take_step\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         n_samples, n_features = X.shape\n",
    "\n",
    "#         # Compute the kernel matrix\n",
    "#         K = np.zeros((n_samples, len(self.support_vectors_)))\n",
    "#         for i in range(n_samples):\n",
    "#             for j in range(len(self.support_vectors_)):\n",
    "#                 K[i,j] = self.kernel_function(X[i], self.support_vectors_[j])\n",
    "\n",
    "#         # Make predictions\n",
    "#         y_pred = np.sum(self.lagrange_multipliers_ * self.support_vectors_y_ * K, axis=1) + self.intercept_\n",
    "\n",
    "#         return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelEncoder = LabelEncoder()\n",
    "transmissionEncoder = LabelEncoder()\n",
    "fuelTypeEncoder = LabelEncoder()\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "def dataset(brand):\n",
    "    file = pd.read_csv(brand, quotechar='\"', skipinitialspace=True)\n",
    "\n",
    "    modelEncoder.fit(file[\"model\"])\n",
    "    file[\"model\"] = modelEncoder.transform(file[\"model\"])\n",
    "    \n",
    "    transmissionEncoder.fit(file[\"transmission\"])\n",
    "    file[\"transmission\"] = transmissionEncoder.transform(file[\"transmission\"])\n",
    "    \n",
    "    fuelTypeEncoder.fit(file[\"fuelType\"])\n",
    "    file[\"fuelType\"] = fuelTypeEncoder.transform(file[\"fuelType\"])\n",
    "\n",
    "    file = file.head(1000) # Limits dataset size\n",
    "\n",
    "    X = file.drop(columns = ['price'])\n",
    "    Y = file.price\n",
    "\n",
    "#     print(file)\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 601)\n",
    "    \n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    print(list(fuelTypeEncoder.classes_))\n",
    "\n",
    "    return  X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Diesel', 'Hybrid', 'Petrol']\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = dataset(\"UKUsedCarDataSet/audi.csv\") # Use Audi dataset as default for KNN analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(test, pred):\n",
    "    print(\"RMSE: \")\n",
    "    MSE = np.square(np.subtract(test, pred)).mean()\n",
    "    return sqrt(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SVMRegression' object has no attribute 'examine_example'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rb22\\Documents\\Rizwan\\Uni\\Year 3\\FYP\\PROJECT\\SVM.ipynb Cell 6\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rb22/Documents/Rizwan/Uni/Year%203/FYP/PROJECT/SVM.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m SVM \u001b[39m=\u001b[39m SVMRegression()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rb22/Documents/Rizwan/Uni/Year%203/FYP/PROJECT/SVM.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m SVM\u001b[39m.\u001b[39;49mfit( X_train, Y_train )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rb22/Documents/Rizwan/Uni/Year%203/FYP/PROJECT/SVM.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m Y_pred \u001b[39m=\u001b[39m SVM\u001b[39m.\u001b[39mpredict( X_test )\n",
      "\u001b[1;32mc:\\Users\\rb22\\Documents\\Rizwan\\Uni\\Year 3\\FYP\\PROJECT\\SVM.ipynb Cell 6\u001b[0m in \u001b[0;36mSVMRegression.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rb22/Documents/Rizwan/Uni/Year%203/FYP/PROJECT/SVM.ipynb#W6sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mif\u001b[39;00m examine_all:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rb22/Documents/Rizwan/Uni/Year%203/FYP/PROJECT/SVM.ipynb#W6sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_samples):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rb22/Documents/Rizwan/Uni/Year%203/FYP/PROJECT/SVM.ipynb#W6sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m         num_changed \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexamine_example(i, X, y, K, lagrange_multipliers, error_cache)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rb22/Documents/Rizwan/Uni/Year%203/FYP/PROJECT/SVM.ipynb#W6sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rb22/Documents/Rizwan/Uni/Year%203/FYP/PROJECT/SVM.ipynb#W6sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     non_bound_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere((lagrange_multipliers \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m&\u001b[39m (lagrange_multipliers \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mC))[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SVMRegression' object has no attribute 'examine_example'"
     ]
    }
   ],
   "source": [
    "SVM = SVMRegression()\n",
    "SVM.fit( X_train, Y_train )\n",
    "Y_pred = SVM.predict( X_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(train, test, yTrain, yTest):    \n",
    "    y_pred =  model.predict(X_test)\n",
    "    error = rmse(yTest, y_pred) \n",
    "    print('The RMSE value is:', error)\n",
    "    for i in range(len(y_pred)):\n",
    "        print(\"\\nOriginal value:\", yTest.iloc[i], \"vs the predicted value:\", y_pred[i])\n",
    "        print(\"The difference is:\", yTest.iloc[i] - y_pred[i])\n",
    "evaluation(X_train, X_test, Y_train, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f25d21d07d980f3bfa83b1b8e5d869424fcc71a3330182c8ba45597eb2457e05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
